{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qSK07vLxKOEU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiSQ0sm5KU49",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.current_device())\n",
        "  print(torch.cuda.device(0))\n",
        "  print(torch.cuda.device_count())\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print(\"No NVIDIA driver found. Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTjkNxNtKd9i"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Z9XOE1J8KmFQ"
      },
      "outputs": [],
      "source": [
        "# Define the colorization model\n",
        "class ColorizationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv4 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = torch.sigmoid(self.conv4(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rkPwdCQwKsQk"
      },
      "outputs": [],
      "source": [
        "# Convert RGB image to grayscale\n",
        "def rgb_to_gray(img):\n",
        "    return img.mean(dim=1, keepdim=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        # Use VGG19 as feature extractor\n",
        "        vgg = torchvision.models.vgg19(pretrained=True).features.to(device)\n",
        "        self.feature_extractor = nn.Sequential()\n",
        "\n",
        "        # Use specific layers from VGG19\n",
        "        self.layers = [0, 5, 10, 19, 28]  # Conv1_1, Conv2_1, Conv3_1, Conv4_1, Conv5_1\n",
        "\n",
        "        # Freeze the network\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Extract the needed layers\n",
        "        i = 0\n",
        "        for layer in vgg.children():\n",
        "            if i <= max(self.layers):\n",
        "                self.feature_extractor.add_module(str(i), layer)\n",
        "            i += 1\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        features = []\n",
        "        for i, layer in enumerate(self.feature_extractor):\n",
        "            x = layer(x)\n",
        "            if i in self.layers:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Normalize inputs to match VGG expected range\n",
        "        pred = pred * 2 - 1  # Convert from [0,1] to [-1,1]\n",
        "        target = target * 2 - 1\n",
        "\n",
        "        pred_features = self.extract_features(pred)\n",
        "        target_features = self.extract_features(target)\n",
        "\n",
        "        # Calculate L1 loss for each feature layer\n",
        "        loss = 0\n",
        "        for pred_feature, target_feature in zip(pred_features, target_features):\n",
        "            loss += nn.functional.l1_loss(pred_feature, target_feature)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "CMRxTiXmQ3TU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training function\n",
        "def train_model(loss_type, epochs=30):\n",
        "    # Initialize model\n",
        "    model = ColorizationNet().to(device)\n",
        "\n",
        "    # Set loss function based on type\n",
        "    if loss_type == 'mse':\n",
        "        criterion = nn.MSELoss()\n",
        "        print(\"Using MSE Loss\")\n",
        "    elif loss_type == 'mae':\n",
        "        criterion = nn.L1Loss()\n",
        "        print(\"Using MAE Loss (L1)\")\n",
        "    elif loss_type == 'perceptual':\n",
        "        criterion = PerceptualLoss()\n",
        "        print(\"Using Perceptual Loss\")\n",
        "    else:\n",
        "        raise ValueError(\"Invalid loss type. Choose 'mse', 'mae', or 'perceptual'\")\n",
        "\n",
        "    # Set optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for i, (images, _) in enumerate(train_loader):\n",
        "            grayscale_images = rgb_to_gray(images).to(device)\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(grayscale_images)\n",
        "            loss = criterion(outputs, images)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Finished Training with {loss_type} loss.\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f\"colorization_model_{loss_type}.pth\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gOyHy04vP5hs"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_on_custom_image(models_dict, image_path):\n",
        "    # Load and preprocess the image\n",
        "    img = Image.open(image_path)\n",
        "    gray_img = img.convert(\"L\")\n",
        "\n",
        "    # Transform to tensor\n",
        "    transform = transforms.Compose([\n",
        "          # Resize to match CIFAR-10 dimensions\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    img_tensor = transform(gray_img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Process with each model\n",
        "    for loss_type, model in models_dict.items():\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            colorized_tensor = model(img_tensor)\n",
        "            results[loss_type] = colorized_tensor.squeeze(0).cpu()\n",
        "\n",
        "    # Visualize results\n",
        "    fig, axes = plt.subplots(1, len(models_dict) + 2, figsize=(4 * (len(models_dict) + 2), 4))\n",
        "\n",
        "    # Original color image\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].set_title(\"Original Color\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Grayscale image\n",
        "    axes[1].imshow(gray_img, cmap='gray')\n",
        "    axes[1].set_title(\"Grayscale\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Colorized images\n",
        "    for i, (loss_type, result) in enumerate(results.items()):\n",
        "        axes[i + 2].imshow(transforms.ToPILImage()(result))\n",
        "        axes[i + 2].set_title(f\"{loss_type.upper()} Loss\")\n",
        "        axes[i + 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save colorized images\n",
        "    for loss_type, result in results.items():\n",
        "        colorized_img = transforms.ToPILImage()(result)\n",
        "        colorized_img.save(f\"colorized_{loss_type}.jpg\")\n",
        "\n",
        "EPOCHS = 30\n",
        "loss_types = ['mse', 'mae', 'perceptual']\n",
        "models = {}\n",
        "\n",
        "for loss_type in loss_types:\n",
        "    print(f\"\\nTraining model with {loss_type} loss\")\n",
        "    model = train_model(loss_type, epochs=EPOCHS)\n",
        "    models[loss_type] = model\n"
      ],
      "metadata": {
        "id": "wrKYkmxRTPS0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1Klmj5iKK3ZU"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    # Convert from Tensor image and display\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if len(img.shape) == 2:  # grayscale image\n",
        "        plt.imshow(npimg, cmap='gray')\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualize_all_three(original_images, grayscale_images, colorized_images, n=5):\n",
        "    \"\"\"\n",
        "    Display grayscale, colorized, and original images side by side.\n",
        "    n: number of images to display from the batch\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(3*n, 4))\n",
        "    for i in range(n):\n",
        "        # Display original image\n",
        "        ax = plt.subplot(1, 3*n, 3*i + 1)\n",
        "        imshow(original_images[i])\n",
        "        ax.set_title(\"Original\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Display original grayscale image\n",
        "        ax = plt.subplot(1, 3*n, 3*i + 2)\n",
        "        imshow(grayscale_images[i])\n",
        "        ax.set_title(\"Grayscale\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Display colorized image\n",
        "        ax = plt.subplot(1, 3*n, 3*i + 3)\n",
        "        imshow(colorized_images[i])\n",
        "        ax.set_title(\"Colorized\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def torch_rgb_to_hsv(rgb):\n",
        "    \"\"\"\n",
        "    Convert an RGB image tensor to HSV.\n",
        "\n",
        "    Parameters:\n",
        "    - rgb: tensor of shape (batch_size, 3, height, width) in RGB format in the range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "    - hsv: tensor of same shape in HSV format in the range [0, 1].\n",
        "    \"\"\"\n",
        "    r, g, b = rgb[:, 0, :, :], rgb[:, 1, :, :], rgb[:, 2, :, :]\n",
        "    max_val, _ = torch.max(rgb, dim=1)\n",
        "    min_val, _ = torch.min(rgb, dim=1)\n",
        "    diff = max_val - min_val\n",
        "\n",
        "    # Compute H\n",
        "    h = torch.zeros_like(r)\n",
        "    mask = (max_val == r) & (g >= b)\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask]\n",
        "    mask = (max_val == r) & (g < b)\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask] + 6.0\n",
        "    mask = max_val == g\n",
        "    h[mask] = (b[mask] - r[mask]) / diff[mask] + 2.0\n",
        "    mask = max_val == b\n",
        "    h[mask] = (r[mask] - g[mask]) / diff[mask] + 4.0\n",
        "    h = h / 6.0\n",
        "    h[diff == 0.0] = 0.0\n",
        "\n",
        "    # Compute S\n",
        "    s = torch.zeros_like(r)\n",
        "    s[diff != 0.0] = diff[diff != 0.0] / max_val[diff != 0.0]\n",
        "\n",
        "    # V is just max_val\n",
        "    v = max_val\n",
        "\n",
        "    return torch.stack([h, s, v], dim=1)\n",
        "\n",
        "\n",
        "def torch_hsv_to_rgb(hsv):\n",
        "    \"\"\"\n",
        "    Convert an HSV image tensor to RGB.\n",
        "\n",
        "    Parameters:\n",
        "    - hsv: tensor of shape (batch_size, 3, height, width) in HSV format in the range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "    - rgb: tensor of same shape in RGB format in the range [0, 1].\n",
        "    \"\"\"\n",
        "    h, s, v = hsv[:, 0, :, :], hsv[:, 1, :, :], hsv[:, 2, :, :]\n",
        "    i = (h * 6.0).floor()\n",
        "    f = h * 6.0 - i\n",
        "    p = v * (1.0 - s)\n",
        "    q = v * (1.0 - s * f)\n",
        "    t = v * (1.0 - s * (1.0 - f))\n",
        "\n",
        "    i_mod = i % 6\n",
        "    r = torch.zeros_like(h)\n",
        "    g = torch.zeros_like(h)\n",
        "    b = torch.zeros_like(h)\n",
        "\n",
        "    r[i_mod == 0.0] = v[i_mod == 0.0]\n",
        "    g[i_mod == 0.0] = t[i_mod == 0.0]\n",
        "    b[i_mod == 0.0] = p[i_mod == 0.0]\n",
        "\n",
        "    r[i_mod == 1.0] = q[i_mod == 1.0]\n",
        "    g[i_mod == 1.0] = v[i_mod == 1.0]\n",
        "    b[i_mod == 1.0] = p[i_mod == 1.0]\n",
        "\n",
        "    r[i_mod == 2.0] = p[i_mod == 2.0]\n",
        "    g[i_mod == 2.0] = v[i_mod == 2.0]\n",
        "    b[i_mod == 2.0] = t[i_mod == 2.0]\n",
        "\n",
        "    r[i_mod == 3.0] = p[i_mod == 3.0]\n",
        "    g[i_mod == 3.0] = q[i_mod == 3.0]\n",
        "    b[i_mod == 3.0] = v[i_mod == 3.0]\n",
        "\n",
        "    r[i_mod == 4.0] = t[i_mod == 4.0]\n",
        "    g[i_mod == 4.0] = p[i_mod == 4.0]\n",
        "    b[i_mod == 4.0] = v[i_mod == 4.0]\n",
        "\n",
        "    r[i_mod == 5.0] = v[i_mod == 5.0]\n",
        "    g[i_mod == 5.0] = p[i_mod == 5.0]\n",
        "    b[i_mod == 5.0] = q[i_mod == 5.0]\n",
        "\n",
        "    return torch.stack([r, g, b], dim=1)\n",
        "\n",
        "def exaggerate_colors(images, saturation_factor=1.5, value_factor=1.2):\n",
        "    \"\"\"\n",
        "    Exaggerate the colors of RGB images.\n",
        "\n",
        "    Parameters:\n",
        "    - images: tensor of shape (batch_size, 3, height, width) in RGB format.\n",
        "    - saturation_factor: factor by which to increase the saturation. Default is 1.5.\n",
        "    - value_factor: factor by which to increase the value/brightness. Default is 1.2.\n",
        "\n",
        "    Returns:\n",
        "    - color_exaggerated_images: tensor of same shape as input, with exaggerated colors.\n",
        "    \"\"\"\n",
        "    # Convert images to the range [0, 1]\n",
        "    images = (images + 1) / 2.0\n",
        "\n",
        "    # Convert RGB images to HSV\n",
        "    images_hsv = torch_rgb_to_hsv(images)\n",
        "\n",
        "    # Increase the saturation and value components\n",
        "    images_hsv[:, 1, :, :] = torch.clamp(images_hsv[:, 1, :, :] * saturation_factor, 0, 1)\n",
        "    images_hsv[:, 2, :, :] = torch.clamp(images_hsv[:, 2, :, :] * value_factor, 0, 1)\n",
        "\n",
        "    # Convert the modified HSV images back to RGB\n",
        "    color_exaggerated_images = torch_hsv_to_rgb(images_hsv)\n",
        "\n",
        "    # Convert images back to the range [-1, 1]\n",
        "    color_exaggerated_images = color_exaggerated_images * 2.0 - 1.0\n",
        "\n",
        "    return color_exaggerated_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Kx3P-w_UN6KR"
      },
      "outputs": [],
      "source": [
        "def test_model(model, loss_type, num_batches=5):\n",
        "    model.eval()\n",
        "    test_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, _) in enumerate(test_loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "            grayscale_images = rgb_to_gray(images).to(device)\n",
        "            colorized_images = model(grayscale_images)\n",
        "\n",
        "            # Convert to CPU for visualization\n",
        "            grayscale_images_cpu = grayscale_images.cpu().squeeze(1)\n",
        "            colorized_images_cpu = colorized_images.cpu()\n",
        "            original_images_cpu = images.cpu()\n",
        "\n",
        "            # Apply color enhancement\n",
        "            colorized_images_cpu_enhanced = exaggerate_colors(colorized_images_cpu)\n",
        "\n",
        "            visualize_all_three(original_images_cpu, grayscale_images_cpu, colorized_images_cpu)\n",
        "            # Store the example for later visualization\n",
        "            test_examples.append({\n",
        "                'original': original_images_cpu,\n",
        "                'grayscale': grayscale_images_cpu,\n",
        "                'colorized': colorized_images_cpu,\n",
        "                'colorized_enhanced': colorized_images_cpu_enhanced\n",
        "            })\n",
        "\n",
        "    return test_examples\n",
        "\n",
        "\n",
        "\n",
        "# Visualize results from all models\n",
        "def visualize_comparison(examples_dict, sample_index=0, image_index=0):\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # Original image\n",
        "    original = examples_dict['mse'][sample_index]['original'][image_index]\n",
        "    grayscale = examples_dict['mse'][sample_index]['grayscale'][image_index]\n",
        "\n",
        "    # Plot original and grayscale\n",
        "    axes[0].imshow(np.transpose(original.numpy(), (1, 2, 0)))\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Plot colorized images for each loss function\n",
        "    for i, loss_type in enumerate(['mse', 'mae', 'perceptual']):\n",
        "        colorized = examples_dict[loss_type][sample_index]['colorized'][image_index]\n",
        "        axes[i+1].imshow(np.transpose(colorized.numpy(), (1, 2, 0)))\n",
        "        axes[i+1].set_title(f\"{loss_type.upper()} Loss\")\n",
        "        axes[i+1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWGK8QhXGj9F",
        "outputId": "e0504b72-3e4e-45d0-9c42-92bdf3ad32be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory:\", current_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln2mIcWjGzdw"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# List the uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    print(\"Uploaded file:\", filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VcIY96J4btnp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Open the image. (Keep your image in the current directory. In my case, the image was horse.jpg)\n",
        "img = Image.open(filename)\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray_img = img.convert(\"L\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test models on the test dataset\n",
        "test_examples = {}\n",
        "\n",
        "for loss_type, model in models.items():\n",
        "    print(f\"\\nTesting model with {loss_type} loss\")\n",
        "    examples= test_model(model, loss_type)\n",
        "    test_examples[loss_type] = examples\n",
        "\n",
        "\n",
        "# Visualize sample results from each model\n",
        "for i in range(min(3, len(test_examples['mse']))):\n",
        "    visualize_comparison(test_examples, sample_index=i, image_index=0)\n",
        "\n",
        "try:\n",
        "    # You can replace this with your own image path\n",
        "    custom_image_path = filename\n",
        "    test_on_custom_image(models, custom_image_path)\n",
        "except Exception as e:\n",
        "    print(f\"Could not test on custom image: {e}\")\n",
        "    print(\"Please upload a custom image and specify the correct path to test.\")"
      ],
      "metadata": {
        "id": "jXd8M_Ex6gUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}