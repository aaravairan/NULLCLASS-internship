{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCpjkwPivtEu",
        "outputId": "883f8193-9ddd-4fec-bb3e-5d379b491937"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.27.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.9.0 (from gradio)\n",
            "  Downloading gradio_client-1.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.27.0-py3-none-any.whl (54.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.9.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.27.0 gradio-client-1.9.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datetime\n",
        "import gradio as gr\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.current_device())\n",
        "  print(torch.cuda.device(0))\n",
        "  print(torch.cuda.device_count())\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print(\"No NVIDIA driver found. Using CPU\")\n",
        "\n",
        "# Define dataset for historical images\n",
        "class HistoricalDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None, period_labels=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_list = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "        # Dictionary mapping image filenames to period labels (e.g., \"1900s\", \"1950s\")\n",
        "        self.period_labels = period_labels if period_labels else {}\n",
        "\n",
        "        # Default period mapping (if not provided explicitly)\n",
        "        self.default_periods = {\n",
        "            \"1850-1900\": 0,\n",
        "            \"1900-1940\": 1,\n",
        "            \"1940-1970\": 2,\n",
        "            \"1970-2000\": 3,\n",
        "            \"2000-present\": 4\n",
        "        }\n",
        "\n",
        "        # Extract year from filename if possible\n",
        "        if not period_labels:\n",
        "            for img_name in self.image_list:\n",
        "                # Try to extract year from filename (assuming format like \"1942_battle.jpg\")\n",
        "                try:\n",
        "                    year_str = ''.join([c for c in img_name.split('_')[0] if c.isdigit()])\n",
        "                    if year_str and len(year_str) == 4:\n",
        "                        year = int(year_str)\n",
        "                        if 1850 <= year < 1900:\n",
        "                            self.period_labels[img_name] = 0\n",
        "                        elif 1900 <= year < 1940:\n",
        "                            self.period_labels[img_name] = 1\n",
        "                        elif 1940 <= year < 1970:\n",
        "                            self.period_labels[img_name] = 2\n",
        "                        elif 1970 <= year < 2000:\n",
        "                            self.period_labels[img_name] = 3\n",
        "                        elif 2000 <= year:\n",
        "                            self.period_labels[img_name] = 4\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_list[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        color_img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            color_img = self.transform(color_img)\n",
        "\n",
        "        # Get period label if available, default to 0 (1850-1900) if not found\n",
        "        period = self.period_labels.get(img_name, 0)\n",
        "\n",
        "        return color_img, period\n",
        "\n",
        "# CIFAR-10 based feature extractor (encoder)\n",
        "class CIFAR10Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        return x\n",
        "\n",
        "# Time Period Classifier Network\n",
        "class TimePeriodClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(TimePeriodClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Enhanced Colorization Network with CIFAR-10 pretrained encoder and Era-Specific Pathways\n",
        "class EnhancedColorizationNet(nn.Module):\n",
        "    def __init__(self, num_eras=5, pretrained_encoder=None):\n",
        "        super(EnhancedColorizationNet, self).__init__()\n",
        "\n",
        "        # The encoder base from CIFAR-10 model\n",
        "        if pretrained_encoder:\n",
        "            self.encoder = pretrained_encoder\n",
        "        else:\n",
        "            self.encoder = CIFAR10Encoder()\n",
        "\n",
        "        # Era-specific pathways (decoders)\n",
        "        self.era_pathways = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "                nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "                nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
        "                nn.Sigmoid()\n",
        "            ) for _ in range(num_eras)\n",
        "        ])\n",
        "\n",
        "        # Era classifier branch\n",
        "        self.era_classifier = TimePeriodClassifier(num_eras)\n",
        "\n",
        "    def forward(self, x, era_idx=None):\n",
        "        # Shared feature extraction through the encoder\n",
        "        x_features = self.encoder(x)\n",
        "\n",
        "        if era_idx is None:\n",
        "            # If era_idx is not provided, use the classifier to predict it\n",
        "            # Create a downsized version for the classifier\n",
        "            x_small = nn.functional.interpolate(x, size=(32, 32), mode='bilinear', align_corners=True)\n",
        "            era_logits = self.era_classifier(x_small)\n",
        "            era_probs = torch.softmax(era_logits, dim=1)\n",
        "            _, era_idx = torch.max(era_probs, dim=1)\n",
        "\n",
        "            # Initialize output tensor\n",
        "            outputs = []\n",
        "            for i in range(len(self.era_pathways)):\n",
        "                # Apply each era's colorization pathway\n",
        "                era_output = self.era_pathways[i](x_features)\n",
        "                outputs.append(era_output)\n",
        "\n",
        "            # Stack outputs\n",
        "            stacked_outputs = torch.stack(outputs, dim=1)\n",
        "\n",
        "            # Create batch indices\n",
        "            batch_size = x.size(0)\n",
        "            batch_indices = torch.arange(batch_size, device=x.device)\n",
        "\n",
        "            # Get era-specific outputs\n",
        "            final_output = stacked_outputs[batch_indices, era_idx]\n",
        "\n",
        "            return final_output, era_idx\n",
        "        else:\n",
        "            # If era_idx is provided as an integer, use that specific pathway\n",
        "            if isinstance(era_idx, int):\n",
        "                return self.era_pathways[era_idx](x_features), torch.tensor([era_idx], device=x.device)\n",
        "            # If era_idx is a tensor, use the provided indices\n",
        "            else:\n",
        "                outputs = []\n",
        "                for i in range(len(self.era_pathways)):\n",
        "                    era_output = self.era_pathways[i](x_features)\n",
        "                    outputs.append(era_output)\n",
        "\n",
        "                stacked_outputs = torch.stack(outputs, dim=1)\n",
        "                batch_size = x.size(0)\n",
        "                batch_indices = torch.arange(batch_size, device=x.device)\n",
        "\n",
        "                final_output = stacked_outputs[batch_indices, era_idx]\n",
        "                return final_output, era_idx\n",
        "\n",
        "# Helper functions\n",
        "def rgb_to_gray(img):\n",
        "    \"\"\"Convert RGB image to grayscale.\"\"\"\n",
        "    return img.mean(dim=1, keepdim=True)\n",
        "\n",
        "def torch_rgb_to_hsv(rgb):\n",
        "    \"\"\"Convert RGB image tensor to HSV format.\"\"\"\n",
        "    r, g, b = rgb[:, 0, :, :], rgb[:, 1, :, :], rgb[:, 2, :, :]\n",
        "    max_val, _ = torch.max(rgb, dim=1)\n",
        "    min_val, _ = torch.min(rgb, dim=1)\n",
        "    diff = max_val - min_val\n",
        "\n",
        "    # Compute H\n",
        "    h = torch.zeros_like(r)\n",
        "    mask = (max_val == r) & (g >= b)\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask]\n",
        "    mask = (max_val == r) & (g < b)\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask] + 6.0\n",
        "    mask = max_val == g\n",
        "    h[mask] = (b[mask] - r[mask]) / diff[mask] + 2.0\n",
        "    mask = max_val == b\n",
        "    h[mask] = (r[mask] - g[mask]) / diff[mask] + 4.0\n",
        "    h = h / 6.0\n",
        "    h[diff == 0.0] = 0.0\n",
        "\n",
        "    # Compute S\n",
        "    s = torch.zeros_like(r)\n",
        "    s[diff != 0.0] = diff[diff != 0.0] / max_val[diff != 0.0]\n",
        "\n",
        "    # V is just max_val\n",
        "    v = max_val\n",
        "\n",
        "    return torch.stack([h, s, v], dim=1)\n",
        "\n",
        "def torch_hsv_to_rgb(hsv):\n",
        "    \"\"\"Convert HSV image tensor to RGB format.\"\"\"\n",
        "    h, s, v = hsv[:, 0, :, :], hsv[:, 1, :, :], hsv[:, 2, :, :]\n",
        "    i = (h * 6.0).floor()\n",
        "    f = h * 6.0 - i\n",
        "    p = v * (1.0 - s)\n",
        "    q = v * (1.0 - s * f)\n",
        "    t = v * (1.0 - s * (1.0 - f))\n",
        "\n",
        "    i_mod = i % 6\n",
        "    r = torch.zeros_like(h)\n",
        "    g = torch.zeros_like(h)\n",
        "    b = torch.zeros_like(h)\n",
        "\n",
        "    r[i_mod == 0.0] = v[i_mod == 0.0]\n",
        "    g[i_mod == 0.0] = t[i_mod == 0.0]\n",
        "    b[i_mod == 0.0] = p[i_mod == 0.0]\n",
        "\n",
        "    r[i_mod == 1.0] = q[i_mod == 1.0]\n",
        "    g[i_mod == 1.0] = v[i_mod == 1.0]\n",
        "    b[i_mod == 1.0] = p[i_mod == 1.0]\n",
        "\n",
        "    r[i_mod == 2.0] = p[i_mod == 2.0]\n",
        "    g[i_mod == 2.0] = v[i_mod == 2.0]\n",
        "    b[i_mod == 2.0] = t[i_mod == 2.0]\n",
        "\n",
        "    r[i_mod == 3.0] = p[i_mod == 3.0]\n",
        "    g[i_mod == 3.0] = q[i_mod == 3.0]\n",
        "    b[i_mod == 3.0] = v[i_mod == 3.0]\n",
        "\n",
        "    r[i_mod == 4.0] = t[i_mod == 4.0]\n",
        "    g[i_mod == 4.0] = p[i_mod == 4.0]\n",
        "    b[i_mod == 4.0] = v[i_mod == 4.0]\n",
        "\n",
        "    r[i_mod == 5.0] = v[i_mod == 5.0]\n",
        "    g[i_mod == 5.0] = p[i_mod == 5.0]\n",
        "    b[i_mod == 5.0] = q[i_mod == 5.0]\n",
        "\n",
        "    return torch.stack([r, g, b], dim=1)\n",
        "\n",
        "def apply_era_specific_enhancements(images, era_idx):\n",
        "    \"\"\"\n",
        "    Apply era-specific color enhancements\n",
        "\n",
        "    Parameters:\n",
        "    - images: tensor of shape (batch_size, 3, height, width)\n",
        "    - era_idx: tensor of shape (batch_size) with values 0-4 representing the era\n",
        "\n",
        "    Returns:\n",
        "    - enhanced_images: tensor with era-specific color adjustments\n",
        "    \"\"\"\n",
        "    # Convert to [0,1] range\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "\n",
        "    # Convert to HSV\n",
        "    images_hsv = torch_rgb_to_hsv(images)\n",
        "    batch_size = images.size(0)\n",
        "\n",
        "    # Era-specific enhancements\n",
        "    # 1850-1900: Sepia-like, brownish tones\n",
        "    # 1900-1940: Lower saturation, slightly bluish\n",
        "    # 1940-1970: Kodachrome look - vibrant but distinct palette\n",
        "    # 1970-2000: Slightly oversaturated with warm tones\n",
        "    # 2000-present: Modern digital look - balanced, accurate colors\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        if era_idx[i] == 0:  # 1850-1900\n",
        "            # Sepia effect\n",
        "            images_hsv[i, 0, :, :] = 0.08  # Hue shift toward yellow-brown\n",
        "            images_hsv[i, 1, :, :] *= 0.7  # Lower saturation\n",
        "            images_hsv[i, 2, :, :] = torch.clamp(images_hsv[i, 2, :, :] * 0.9, 0, 1)  # Slightly darker\n",
        "\n",
        "        elif era_idx[i] == 1:  # 1900-1940\n",
        "            # Early film look\n",
        "            images_hsv[i, 0, :, :] = torch.clamp(images_hsv[i, 0, :, :] + 0.05, 0, 1)  # Slight hue shift\n",
        "            images_hsv[i, 1, :, :] *= 0.8  # Lower saturation\n",
        "            # Add slight blue tint to shadows\n",
        "            shadow_mask = images_hsv[i, 2, :, :] < 0.4\n",
        "            images_hsv[i, 0, shadow_mask] = 0.6  # Blue-ish hue\n",
        "\n",
        "        elif era_idx[i] == 2:  # 1940-1970\n",
        "            # Kodachrome look\n",
        "            images_hsv[i, 1, :, :] = torch.clamp(images_hsv[i, 1, :, :] * 1.2, 0, 1)  # More saturation\n",
        "            # Enhance reds and yellows\n",
        "            red_mask = (images_hsv[i, 0, :, :] < 0.05) | (images_hsv[i, 0, :, :] > 0.95)\n",
        "            images_hsv[i, 1, red_mask] = torch.clamp(images_hsv[i, 1, red_mask] * 1.3, 0, 1)\n",
        "\n",
        "        elif era_idx[i] == 3:  # 1970-2000\n",
        "            # Film of the late 20th century\n",
        "            images_hsv[i, 1, :, :] = torch.clamp(images_hsv[i, 1, :, :] * 1.1, 0, 1)  # Slightly increased saturation\n",
        "            images_hsv[i, 0, :, :] = torch.clamp(images_hsv[i, 0, :, :] - 0.02, 0, 1)  # Slight warm shift\n",
        "\n",
        "        # For era_idx 4 (2000-present), we keep colors as they are - digital look\n",
        "\n",
        "    # Convert back to RGB\n",
        "    enhanced_images = torch_hsv_to_rgb(images_hsv)\n",
        "    return enhanced_images\n",
        "\n",
        "# Function to load CIFAR-10 pretrained model\n",
        "def load_cifar10_pretrained_model(pth_path):\n",
        "    try:\n",
        "        # Load the PyTorch model state dict\n",
        "        cifar_state_dict = torch.load(cifar10_model.pth)\n",
        "\n",
        "        # Create a model instance to load the state dict into\n",
        "        cifar_model = CIFAR10Encoder()\n",
        "        cifar_model.load_state_dict(cifar_state_dict)\n",
        "\n",
        "        print(f\"Successfully loaded CIFAR-10 model from {pth_path}\")\n",
        "        return cifar_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CIFAR-10 model: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract encoder from CIFAR-10 model and adapt it\n",
        "def extract_encoder_from_cifar10(cifar_model):\n",
        "    try:\n",
        "        # Create a new encoder model\n",
        "        encoder = CIFAR10Encoder()\n",
        "\n",
        "        # Try to map weights from CIFAR-10 model to our encoder\n",
        "        # This is a placeholder - actual implementation depends on the structure of your pickle file\n",
        "\n",
        "        # Option 1: If the pickle contains a state_dict\n",
        "        if hasattr(cifar_model, 'state_dict'):\n",
        "            # Map relevant layers\n",
        "            encoder_dict = encoder.state_dict()\n",
        "            cifar_dict = cifar_model.state_dict()\n",
        "\n",
        "            # Mapping will depend on the actual structure of your CIFAR-10 model\n",
        "            # This is a simplified example:\n",
        "            for name, param in cifar_dict.items():\n",
        "                if 'conv1' in name and name.replace('conv1', '') in encoder_dict:\n",
        "                    encoder_dict[name] = param\n",
        "\n",
        "            encoder.load_state_dict(encoder_dict)\n",
        "            print(\"Loaded weights from CIFAR-10 model state_dict\")\n",
        "\n",
        "        # Option 2: If the pickle is the model itself or has a different structure\n",
        "        elif hasattr(cifar_model, 'conv1'):\n",
        "            # Transfer weights directly if the architecture is compatible\n",
        "            encoder.conv1.weight.data = cifar_model.conv1.weight.data\n",
        "            encoder.conv1.bias.data = cifar_model.conv1.bias.data\n",
        "            # Similarly for other layers...\n",
        "            print(\"Transferred weights directly from CIFAR-10 model attributes\")\n",
        "\n",
        "        else:\n",
        "            print(\"CIFAR-10 model structure not recognized, using random initialization\")\n",
        "\n",
        "        return encoder\n",
        "    except Exception as e:\n",
        "        print(f\"Error adapting CIFAR-10 model: {str(e)}\")\n",
        "        print(\"Returning an untrained encoder\")\n",
        "        return CIFAR10Encoder()\n",
        "\n",
        "# Training function for CIFAR-10 dataset (in case we need to train from scratch)\n",
        "def train_on_cifar10(encoder, num_epochs=40):\n",
        "    print(\"Starting CIFAR-10 training...\")\n",
        "\n",
        "    # Data loading for CIFAR-10\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-10 training set\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    # Setup a simple classifier on top of the encoder for training\n",
        "    class CIFAR10Classifier(nn.Module):\n",
        "        def __init__(self, encoder):\n",
        "            super(CIFAR10Classifier, self).__init__()\n",
        "            self.encoder = encoder\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "            self.fc = nn.Linear(256, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # CIFAR is RGB, our encoder expects grayscale - convert\n",
        "            x_gray = rgb_to_gray(x)\n",
        "            features = self.encoder(x_gray)\n",
        "            x = self.avgpool(features)\n",
        "            x = torch.flatten(x, 1)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    # Create classifier with the encoder\n",
        "    classifier = CIFAR10Classifier(encoder).to(device)\n",
        "\n",
        "    # Set up loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = classifier(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:\n",
        "                print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished CIFAR-10 Training')\n",
        "\n",
        "    # Save CIFAR-10 trained model if needed\n",
        "    torch.save(encoder.state_dict(), 'cifar10_encoder.pth')\n",
        "\n",
        "    return encoder\n",
        "\n",
        "# Fine-tuning function\n",
        "def fine_tune_model(model, train_loader, optimizer, criterion, num_epochs=40, freeze_encoder=True):\n",
        "    model.train()\n",
        "\n",
        "    # Optionally freeze the encoder layers\n",
        "    if freeze_encoder:\n",
        "        for param in model.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (images, period_labels) in enumerate(train_loader):\n",
        "            grayscale_images = rgb_to_gray(images).to(device)\n",
        "            images = images.to(device)\n",
        "            period_labels = period_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with era labels\n",
        "            outputs, predicted_era = model(grayscale_images, period_labels)\n",
        "            loss = criterion(outputs, images)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 20 == 19:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/20:.4f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print(\"Finished Fine-tuning\")\n",
        "    return model\n",
        "\n",
        "# Era labels\n",
        "era_labels = [\n",
        "    \"1850-1900 (Victorian/Early Photography)\",\n",
        "    \"1900-1940 (Early 20th Century)\",\n",
        "    \"1940-1970 (Mid-Century)\",\n",
        "    \"1970-2000 (Late 20th Century)\",\n",
        "    \"2000-present (Digital Era)\"\n",
        "]\n",
        "\n",
        "# Define the colorization function for Gradio\n",
        "def colorize_image(input_img, era_selection):\n",
        "    global model\n",
        "    model.eval()\n",
        "\n",
        "    # Map era selection string to index\n",
        "    era_idx = era_labels.index(era_selection)\n",
        "\n",
        "    # Convert input image to grayscale\n",
        "    if input_img is None:\n",
        "        return None, None, \"No image provided\"\n",
        "\n",
        "    # Create grayscale version\n",
        "    gray_img = Image.fromarray(input_img).convert(\"L\")\n",
        "\n",
        "    # Transform for model input\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Convert to tensor\n",
        "    gray_tensor = transform(gray_img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Process with the model\n",
        "    with torch.no_grad():\n",
        "        colorized_tensor, predicted_era = model(gray_tensor, era_idx)\n",
        "\n",
        "        # Apply era-specific enhancements\n",
        "        enhanced_tensor = apply_era_specific_enhancements(colorized_tensor, predicted_era)\n",
        "\n",
        "    # Convert tensor back to PIL image and then to numpy array\n",
        "    colorized_img = transforms.ToPILImage()(enhanced_tensor.squeeze(0).cpu())\n",
        "\n",
        "    # Resize back to original dimensions\n",
        "    original_size = (input_img.shape[1], input_img.shape[0])  # Width, Height\n",
        "    colorized_img = colorized_img.resize(original_size)\n",
        "\n",
        "    # Convert to numpy array for Gradio\n",
        "    colorized_array = np.array(colorized_img)\n",
        "\n",
        "    # Create grayscale version for display\n",
        "    gray_array = np.array(Image.fromarray(input_img).convert(\"L\").convert(\"RGB\"))\n",
        "\n",
        "    # Get detected era for display\n",
        "    if isinstance(predicted_era, torch.Tensor) and len(predicted_era) > 0:\n",
        "        detected_era_idx = predicted_era[0].item()\n",
        "        detected_era = f\"Detected Era: {era_labels[detected_era_idx]}\"\n",
        "    else:\n",
        "        detected_era = \"Era detection failed\"\n",
        "\n",
        "    return gray_array, colorized_array, detected_era\n",
        "\n",
        "# Function to initialize dataset and model\n",
        "def prepare_model_and_dataset(cifar10_model_path=None):\n",
        "    global model\n",
        "\n",
        "    # Step 1: Attempt to load the CIFAR-10 pretrained model\n",
        "    cifar10_model = None\n",
        "    encoder = None\n",
        "\n",
        "    if cifar10_model_path:\n",
        "        cifar10_model = load_cifar10_pretrained_model(cifar10_model_path)\n",
        "        if cifar10_model:\n",
        "            # Extract and adapt the encoder from CIFAR-10 model\n",
        "            encoder = extract_encoder_from_cifar10(cifar10_model)\n",
        "        else:\n",
        "            print(\"Failed to load CIFAR-10 model from pickle, will attempt to train on CIFAR-10\")\n",
        "\n",
        "    # If we couldn't load or adapt the CIFAR-10 model, train one from scratch\n",
        "    if encoder is None:\n",
        "        encoder = CIFAR10Encoder()\n",
        "        try:\n",
        "            # Train on CIFAR-10\n",
        "            encoder = train_on_cifar10(encoder)\n",
        "        except Exception as e:\n",
        "            print(f\"CIFAR-10 training failed: {str(e)}\")\n",
        "            print(\"Using untrained encoder\")\n",
        "\n",
        "    # Step 2: Create the full model with the encoder\n",
        "    model = EnhancedColorizationNet(num_eras=5, pretrained_encoder=encoder).to(device)\n",
        "\n",
        "    # Data transformations for historical dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Step 3: Try to load the full model weights if they exist\n",
        "    model_loaded = False\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(\"era_sensitive_colorization_model.pth\"))\n",
        "        print(\"Loaded pre-trained full model weights\")\n",
        "        model_loaded = True\n",
        "    except:\n",
        "        print(\"Pre-trained full model not found, will attempt to fine-tune with historical dataset\")\n",
        "\n",
        "    # Step 4: If we couldn't load the full model, fine-tune with historical dataset\n",
        "    if not model_loaded:\n",
        "        try:\n",
        "            # Create dataset (adjust the path as needed)\n",
        "            train_dataset = HistoricalDataset(\"images\", transform=transform)\n",
        "            train_size = int(0.8 * len(train_dataset))\n",
        "            test_size = len(train_dataset) - train_size\n",
        "            train_data, test_data = random_split(train_dataset, [train_size, test_size])\n",
        "\n",
        "            train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
        "            test_loader = DataLoader(test_data, batch_size=4, shuffle=False)\n",
        "\n",
        "            print(f\"Historical dataset loaded with {len(train_dataset)} images\")\n",
        "            print(f\"Fine-tuning with {train_size} images, testing with {test_size} images\")\n",
        "\n",
        "            # Fine-tune the model\n",
        "            criterion = nn.MSELoss()\n",
        "            # Only train decoder parts initially\n",
        "            optimizer = optim.Adam(\n",
        "                list(model.era_pathways.parameters()) + list(model.era_classifier.parameters()),\n",
        "                lr=0.001\n",
        "            )\n",
        "            model = fine_tune_model(model, train_loader, optimizer, criterion, num_epochs=40, freeze_encoder=True)\n",
        "\n",
        "            # Optional: Unfreeze the encoder for a few more epochs with lower learning rate\n",
        "            for param in model.encoder.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "            model = fine_tune_model(model, train_loader, optimizer, criterion, num_epochs=40, freeze_encoder=False)\n",
        "\n",
        "            # Save the trained model\n",
        "            torch.save(model.state_dict(), \"era_sensitive_colorization_model.pth\")\n",
        "            print(\"Model fine-tuned and saved to era_sensitive_colorization_model.pth\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not fine-tune model: {str(e)}\")\n",
        "            print(\"Using untrained model - results may not be optimal\")\n",
        "\n",
        "    return \"Model preparation complete\"\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    # Initialize global model variable\n",
        "    global model\n",
        "\n",
        "    # Prepare the model and dataset\n",
        "    cifar10_model_path = \"/content/cifar10_model.pth\"  # Changed from .pkl to .pth\n",
        "\n",
        "    # Prepare the model and dataset with both file paths\n",
        "    status = prepare_model_and_dataset(\n",
        "        cifar10_model_path=cifar10_model_path\n",
        "    )\n",
        "    print(status)\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(title=\"Historical Image Colorization\") as demo:\n",
        "        gr.Markdown(\"# Era-Sensitive Historical Image Colorization\")\n",
        "        gr.Markdown(\"Upload a black and white historical photo and select an era for colorization.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                input_image = gr.Image(label=\"Upload B&W Image\", type=\"numpy\")\n",
        "                era_dropdown = gr.Dropdown(\n",
        "                    choices=era_labels,\n",
        "                    value=era_labels[2],  # Default to Mid-Century\n",
        "                    label=\"Select Era for Colorization\"\n",
        "                )\n",
        "                colorize_btn = gr.Button(\"Colorize Image\")\n",
        "\n",
        "            with gr.Column():\n",
        "                grayscale_output = gr.Image(label=\"Grayscale Image\")\n",
        "                colorized_output = gr.Image(label=\"Colorized Result\")\n",
        "                era_info = gr.Textbox(label=\"Era Information\")\n",
        "\n",
        "        # Set up the click event\n",
        "        colorize_btn.click(\n",
        "            fn=colorize_image,\n",
        "            inputs=[input_image, era_dropdown],\n",
        "            outputs=[grayscale_output, colorized_output, era_info]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## About This Tool\n",
        "\n",
        "        This tool uses deep learning to colorize black and white historical photos in a way that's sensitive to different historical time periods:\n",
        "\n",
        "        - **1850-1900**: Victorian-era sepia tones\n",
        "        - **1900-1940**: Early 20th century film look\n",
        "        - **1940-1970**: Kodachrome-inspired mid-century colors\n",
        "        - **1970-2000**: Late 20th century film photography\n",
        "        - **2000-present**: Modern digital photography colors\n",
        "\n",
        "        The model will attempt to detect the appropriate era automatically, but you can also select a specific era for stylistic colorization.\n",
        "        \"\"\")\n",
        "\n",
        "    # Launch the Gradio interface\n",
        "    print(\"Starting Gradio server...\")\n",
        "    demo.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WA0ltVa3tpiA",
        "outputId": "3ff3f8ad-417e-420d-fb97-1d752f47042a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7efaf65bac10>\n",
            "1\n",
            "Tesla T4\n",
            "Error loading CIFAR-10 model: name 'cifar10_model' is not defined\n",
            "Failed to load CIFAR-10 model from pickle, will attempt to train on CIFAR-10\n",
            "Starting CIFAR-10 training...\n",
            "[1, 200] loss: 2.154\n",
            "[1, 400] loss: 1.859\n",
            "[1, 600] loss: 1.745\n",
            "[2, 200] loss: 1.616\n",
            "[2, 400] loss: 1.574\n",
            "[2, 600] loss: 1.507\n",
            "[3, 200] loss: 1.440\n",
            "[3, 400] loss: 1.399\n",
            "[3, 600] loss: 1.378\n",
            "[4, 200] loss: 1.339\n",
            "[4, 400] loss: 1.311\n",
            "[4, 600] loss: 1.289\n",
            "[5, 200] loss: 1.246\n",
            "[5, 400] loss: 1.235\n",
            "[5, 600] loss: 1.208\n",
            "[6, 200] loss: 1.206\n",
            "[6, 400] loss: 1.171\n",
            "[6, 600] loss: 1.163\n",
            "[7, 200] loss: 1.138\n",
            "[7, 400] loss: 1.134\n",
            "[7, 600] loss: 1.114\n",
            "[8, 200] loss: 1.097\n",
            "[8, 400] loss: 1.063\n",
            "[8, 600] loss: 1.100\n",
            "[9, 200] loss: 1.046\n",
            "[9, 400] loss: 1.045\n",
            "[9, 600] loss: 1.058\n",
            "[10, 200] loss: 1.027\n",
            "[10, 400] loss: 1.022\n",
            "[10, 600] loss: 0.997\n",
            "[11, 200] loss: 0.983\n",
            "[11, 400] loss: 0.968\n",
            "[11, 600] loss: 0.975\n",
            "[12, 200] loss: 0.939\n",
            "[12, 400] loss: 0.946\n",
            "[12, 600] loss: 0.967\n",
            "[13, 200] loss: 0.934\n",
            "[13, 400] loss: 0.916\n",
            "[13, 600] loss: 0.923\n",
            "[14, 200] loss: 0.904\n",
            "[14, 400] loss: 0.899\n",
            "[14, 600] loss: 0.892\n",
            "[15, 200] loss: 0.874\n",
            "[15, 400] loss: 0.884\n",
            "[15, 600] loss: 0.865\n",
            "[16, 200] loss: 0.848\n",
            "[16, 400] loss: 0.840\n",
            "[16, 600] loss: 0.847\n",
            "[17, 200] loss: 0.825\n",
            "[17, 400] loss: 0.823\n",
            "[17, 600] loss: 0.825\n",
            "[18, 200] loss: 0.777\n",
            "[18, 400] loss: 0.791\n",
            "[18, 600] loss: 0.807\n",
            "[19, 200] loss: 0.776\n",
            "[19, 400] loss: 0.775\n",
            "[19, 600] loss: 0.787\n",
            "[20, 200] loss: 0.755\n",
            "[20, 400] loss: 0.746\n",
            "[20, 600] loss: 0.760\n",
            "[21, 200] loss: 0.742\n",
            "[21, 400] loss: 0.725\n",
            "[21, 600] loss: 0.743\n",
            "[22, 200] loss: 0.719\n",
            "[22, 400] loss: 0.709\n",
            "[22, 600] loss: 0.720\n",
            "[23, 200] loss: 0.702\n",
            "[23, 400] loss: 0.698\n",
            "[23, 600] loss: 0.693\n",
            "[24, 200] loss: 0.696\n",
            "[24, 400] loss: 0.686\n",
            "[24, 600] loss: 0.682\n",
            "[25, 200] loss: 0.668\n",
            "[25, 400] loss: 0.680\n",
            "[25, 600] loss: 0.664\n",
            "[26, 200] loss: 0.648\n",
            "[26, 400] loss: 0.658\n",
            "[26, 600] loss: 0.646\n",
            "[27, 200] loss: 0.638\n",
            "[27, 400] loss: 0.631\n",
            "[27, 600] loss: 0.641\n",
            "[28, 200] loss: 0.607\n",
            "[28, 400] loss: 0.621\n",
            "[28, 600] loss: 0.631\n",
            "[29, 200] loss: 0.607\n",
            "[29, 400] loss: 0.616\n",
            "[29, 600] loss: 0.604\n",
            "[30, 200] loss: 0.590\n",
            "[30, 400] loss: 0.584\n",
            "[30, 600] loss: 0.600\n",
            "[31, 200] loss: 0.584\n",
            "[31, 400] loss: 0.572\n",
            "[31, 600] loss: 0.566\n",
            "[32, 200] loss: 0.569\n",
            "[32, 400] loss: 0.573\n",
            "[32, 600] loss: 0.562\n",
            "[33, 200] loss: 0.554\n",
            "[33, 400] loss: 0.556\n",
            "[33, 600] loss: 0.556\n",
            "[34, 200] loss: 0.551\n",
            "[34, 400] loss: 0.540\n",
            "[34, 600] loss: 0.535\n",
            "[35, 200] loss: 0.528\n",
            "[35, 400] loss: 0.521\n",
            "[35, 600] loss: 0.536\n",
            "[36, 200] loss: 0.514\n",
            "[36, 400] loss: 0.528\n",
            "[36, 600] loss: 0.526\n",
            "[37, 200] loss: 0.499\n",
            "[37, 400] loss: 0.497\n",
            "[37, 600] loss: 0.516\n",
            "[38, 200] loss: 0.487\n",
            "[38, 400] loss: 0.501\n",
            "[38, 600] loss: 0.512\n",
            "[39, 200] loss: 0.475\n",
            "[39, 400] loss: 0.495\n",
            "[39, 600] loss: 0.491\n",
            "[40, 200] loss: 0.481\n",
            "[40, 400] loss: 0.476\n",
            "[40, 600] loss: 0.481\n",
            "Finished CIFAR-10 Training\n",
            "Pre-trained full model not found, will attempt to fine-tune with historical dataset\n",
            "Historical dataset loaded with 267 images\n",
            "Fine-tuning with 213 images, testing with 54 images\n",
            "Epoch [1/40], Step [20/54], Loss: 0.0261\n",
            "Epoch [1/40], Step [40/54], Loss: 0.0110\n",
            "Epoch [2/40], Step [20/54], Loss: 0.0081\n",
            "Epoch [2/40], Step [40/54], Loss: 0.0080\n",
            "Epoch [3/40], Step [20/54], Loss: 0.0081\n",
            "Epoch [3/40], Step [40/54], Loss: 0.0074\n",
            "Epoch [4/40], Step [20/54], Loss: 0.0075\n",
            "Epoch [4/40], Step [40/54], Loss: 0.0070\n",
            "Epoch [5/40], Step [20/54], Loss: 0.0066\n",
            "Epoch [5/40], Step [40/54], Loss: 0.0068\n",
            "Epoch [6/40], Step [20/54], Loss: 0.0059\n",
            "Epoch [6/40], Step [40/54], Loss: 0.0073\n",
            "Epoch [7/40], Step [20/54], Loss: 0.0071\n",
            "Epoch [7/40], Step [40/54], Loss: 0.0063\n",
            "Epoch [8/40], Step [20/54], Loss: 0.0059\n",
            "Epoch [8/40], Step [40/54], Loss: 0.0061\n",
            "Epoch [9/40], Step [20/54], Loss: 0.0065\n",
            "Epoch [9/40], Step [40/54], Loss: 0.0063\n",
            "Epoch [10/40], Step [20/54], Loss: 0.0068\n",
            "Epoch [10/40], Step [40/54], Loss: 0.0062\n",
            "Epoch [11/40], Step [20/54], Loss: 0.0062\n",
            "Epoch [11/40], Step [40/54], Loss: 0.0065\n",
            "Epoch [12/40], Step [20/54], Loss: 0.0057\n",
            "Epoch [12/40], Step [40/54], Loss: 0.0060\n",
            "Epoch [13/40], Step [20/54], Loss: 0.0060\n",
            "Epoch [13/40], Step [40/54], Loss: 0.0064\n",
            "Epoch [14/40], Step [20/54], Loss: 0.0064\n",
            "Epoch [14/40], Step [40/54], Loss: 0.0064\n",
            "Epoch [15/40], Step [20/54], Loss: 0.0063\n",
            "Epoch [15/40], Step [40/54], Loss: 0.0059\n",
            "Epoch [16/40], Step [20/54], Loss: 0.0056\n",
            "Epoch [16/40], Step [40/54], Loss: 0.0059\n",
            "Epoch [17/40], Step [20/54], Loss: 0.0062\n",
            "Epoch [17/40], Step [40/54], Loss: 0.0058\n",
            "Epoch [18/40], Step [20/54], Loss: 0.0059\n",
            "Epoch [18/40], Step [40/54], Loss: 0.0066\n",
            "Epoch [19/40], Step [20/54], Loss: 0.0065\n",
            "Epoch [19/40], Step [40/54], Loss: 0.0057\n",
            "Epoch [20/40], Step [20/54], Loss: 0.0059\n",
            "Epoch [20/40], Step [40/54], Loss: 0.0063\n",
            "Epoch [21/40], Step [20/54], Loss: 0.0052\n",
            "Epoch [21/40], Step [40/54], Loss: 0.0068\n",
            "Epoch [22/40], Step [20/54], Loss: 0.0065\n",
            "Epoch [22/40], Step [40/54], Loss: 0.0059\n",
            "Epoch [23/40], Step [20/54], Loss: 0.0058\n",
            "Epoch [23/40], Step [40/54], Loss: 0.0058\n",
            "Epoch [24/40], Step [20/54], Loss: 0.0057\n",
            "Epoch [24/40], Step [40/54], Loss: 0.0060\n",
            "Epoch [25/40], Step [20/54], Loss: 0.0058\n",
            "Epoch [25/40], Step [40/54], Loss: 0.0060\n",
            "Epoch [26/40], Step [20/54], Loss: 0.0064\n",
            "Epoch [26/40], Step [40/54], Loss: 0.0063\n",
            "Epoch [27/40], Step [20/54], Loss: 0.0058\n",
            "Epoch [27/40], Step [40/54], Loss: 0.0061\n",
            "Epoch [28/40], Step [20/54], Loss: 0.0059\n",
            "Epoch [28/40], Step [40/54], Loss: 0.0049\n",
            "Epoch [29/40], Step [20/54], Loss: 0.0056\n",
            "Epoch [29/40], Step [40/54], Loss: 0.0053\n",
            "Epoch [30/40], Step [20/54], Loss: 0.0051\n",
            "Epoch [30/40], Step [40/54], Loss: 0.0061\n",
            "Epoch [31/40], Step [20/54], Loss: 0.0058\n",
            "Epoch [31/40], Step [40/54], Loss: 0.0059\n",
            "Epoch [32/40], Step [20/54], Loss: 0.0063\n",
            "Epoch [32/40], Step [40/54], Loss: 0.0060\n",
            "Epoch [33/40], Step [20/54], Loss: 0.0052\n",
            "Epoch [33/40], Step [40/54], Loss: 0.0063\n",
            "Epoch [34/40], Step [20/54], Loss: 0.0051\n",
            "Epoch [34/40], Step [40/54], Loss: 0.0058\n",
            "Epoch [35/40], Step [20/54], Loss: 0.0056\n",
            "Epoch [35/40], Step [40/54], Loss: 0.0061\n",
            "Epoch [36/40], Step [20/54], Loss: 0.0060\n",
            "Epoch [36/40], Step [40/54], Loss: 0.0055\n",
            "Epoch [37/40], Step [20/54], Loss: 0.0060\n",
            "Epoch [37/40], Step [40/54], Loss: 0.0055\n",
            "Epoch [38/40], Step [20/54], Loss: 0.0054\n",
            "Epoch [38/40], Step [40/54], Loss: 0.0052\n",
            "Epoch [39/40], Step [20/54], Loss: 0.0050\n",
            "Epoch [39/40], Step [40/54], Loss: 0.0059\n",
            "Epoch [40/40], Step [20/54], Loss: 0.0056\n",
            "Epoch [40/40], Step [40/54], Loss: 0.0051\n",
            "Finished Fine-tuning\n",
            "Epoch [1/40], Step [20/54], Loss: 0.0053\n",
            "Epoch [1/40], Step [40/54], Loss: 0.0055\n",
            "Epoch [2/40], Step [20/54], Loss: 0.0049\n",
            "Epoch [2/40], Step [40/54], Loss: 0.0049\n",
            "Epoch [3/40], Step [20/54], Loss: 0.0052\n",
            "Epoch [3/40], Step [40/54], Loss: 0.0046\n",
            "Epoch [4/40], Step [20/54], Loss: 0.0048\n",
            "Epoch [4/40], Step [40/54], Loss: 0.0049\n",
            "Epoch [5/40], Step [20/54], Loss: 0.0051\n",
            "Epoch [5/40], Step [40/54], Loss: 0.0051\n",
            "Epoch [6/40], Step [20/54], Loss: 0.0048\n",
            "Epoch [6/40], Step [40/54], Loss: 0.0047\n",
            "Epoch [7/40], Step [20/54], Loss: 0.0051\n",
            "Epoch [7/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [8/40], Step [20/54], Loss: 0.0050\n",
            "Epoch [8/40], Step [40/54], Loss: 0.0053\n",
            "Epoch [9/40], Step [20/54], Loss: 0.0049\n",
            "Epoch [9/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [10/40], Step [20/54], Loss: 0.0042\n",
            "Epoch [10/40], Step [40/54], Loss: 0.0051\n",
            "Epoch [11/40], Step [20/54], Loss: 0.0051\n",
            "Epoch [11/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [12/40], Step [20/54], Loss: 0.0048\n",
            "Epoch [12/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [13/40], Step [20/54], Loss: 0.0046\n",
            "Epoch [13/40], Step [40/54], Loss: 0.0046\n",
            "Epoch [14/40], Step [20/54], Loss: 0.0047\n",
            "Epoch [14/40], Step [40/54], Loss: 0.0048\n",
            "Epoch [15/40], Step [20/54], Loss: 0.0047\n",
            "Epoch [15/40], Step [40/54], Loss: 0.0051\n",
            "Epoch [16/40], Step [20/54], Loss: 0.0046\n",
            "Epoch [16/40], Step [40/54], Loss: 0.0043\n",
            "Epoch [17/40], Step [20/54], Loss: 0.0050\n",
            "Epoch [17/40], Step [40/54], Loss: 0.0048\n",
            "Epoch [18/40], Step [20/54], Loss: 0.0051\n",
            "Epoch [18/40], Step [40/54], Loss: 0.0047\n",
            "Epoch [19/40], Step [20/54], Loss: 0.0043\n",
            "Epoch [19/40], Step [40/54], Loss: 0.0049\n",
            "Epoch [20/40], Step [20/54], Loss: 0.0050\n",
            "Epoch [20/40], Step [40/54], Loss: 0.0047\n",
            "Epoch [21/40], Step [20/54], Loss: 0.0046\n",
            "Epoch [21/40], Step [40/54], Loss: 0.0048\n",
            "Epoch [22/40], Step [20/54], Loss: 0.0047\n",
            "Epoch [22/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [23/40], Step [20/54], Loss: 0.0050\n",
            "Epoch [23/40], Step [40/54], Loss: 0.0046\n",
            "Epoch [24/40], Step [20/54], Loss: 0.0044\n",
            "Epoch [24/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [25/40], Step [20/54], Loss: 0.0049\n",
            "Epoch [25/40], Step [40/54], Loss: 0.0041\n",
            "Epoch [26/40], Step [20/54], Loss: 0.0043\n",
            "Epoch [26/40], Step [40/54], Loss: 0.0048\n",
            "Epoch [27/40], Step [20/54], Loss: 0.0046\n",
            "Epoch [27/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [28/40], Step [20/54], Loss: 0.0048\n",
            "Epoch [28/40], Step [40/54], Loss: 0.0043\n",
            "Epoch [29/40], Step [20/54], Loss: 0.0042\n",
            "Epoch [29/40], Step [40/54], Loss: 0.0042\n",
            "Epoch [30/40], Step [20/54], Loss: 0.0044\n",
            "Epoch [30/40], Step [40/54], Loss: 0.0048\n",
            "Epoch [31/40], Step [20/54], Loss: 0.0050\n",
            "Epoch [31/40], Step [40/54], Loss: 0.0045\n",
            "Epoch [32/40], Step [20/54], Loss: 0.0048\n",
            "Epoch [32/40], Step [40/54], Loss: 0.0040\n",
            "Epoch [33/40], Step [20/54], Loss: 0.0047\n",
            "Epoch [33/40], Step [40/54], Loss: 0.0044\n",
            "Epoch [34/40], Step [20/54], Loss: 0.0044\n",
            "Epoch [34/40], Step [40/54], Loss: 0.0047\n",
            "Epoch [35/40], Step [20/54], Loss: 0.0045\n",
            "Epoch [35/40], Step [40/54], Loss: 0.0047\n",
            "Epoch [36/40], Step [20/54], Loss: 0.0046\n",
            "Epoch [36/40], Step [40/54], Loss: 0.0042\n",
            "Epoch [37/40], Step [20/54], Loss: 0.0041\n",
            "Epoch [37/40], Step [40/54], Loss: 0.0048\n",
            "Epoch [38/40], Step [20/54], Loss: 0.0044\n",
            "Epoch [38/40], Step [40/54], Loss: 0.0048\n",
            "Epoch [39/40], Step [20/54], Loss: 0.0045\n",
            "Epoch [39/40], Step [40/54], Loss: 0.0042\n",
            "Epoch [40/40], Step [20/54], Loss: 0.0044\n",
            "Epoch [40/40], Step [40/54], Loss: 0.0041\n",
            "Finished Fine-tuning\n",
            "Model fine-tuned and saved to era_sensitive_colorization_model.pth\n",
            "Model preparation complete\n",
            "Starting Gradio server...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b3256a1401eefbd71e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b3256a1401eefbd71e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}