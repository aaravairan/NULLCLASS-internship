{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FefpY6wSR1Am"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qSK07vLxKOEU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiSQ0sm5KU49",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.current_device())\n",
        "  print(torch.cuda.device(0))\n",
        "  print(torch.cuda.device_count())\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print(\"No NVIDIA driver found. Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class HistoricalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_list = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_list[idx])\n",
        "        color_img = Image.open(img_path).convert(\"RGB\")  # RGB image\n",
        "\n",
        "        if self.transform:\n",
        "            color_img = self.transform(color_img)\n",
        "\n",
        "        return color_img, 0"
      ],
      "metadata": {
        "id": "k62puFitwc69"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # match your model input size\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = HistoricalDataset(\"/content/images\", transform=transform)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "test_size = len(train_dataset) - train_size\n",
        "train_data, test_data = random_split(train_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "skjyokw9xsI-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z9XOE1J8KmFQ"
      },
      "outputs": [],
      "source": [
        "# Define the colorization model\n",
        "class ColorizationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv4 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = torch.sigmoid(self.conv4(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rkPwdCQwKsQk"
      },
      "outputs": [],
      "source": [
        "model = ColorizationNet().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Convert RGB image to grayscale\n",
        "def rgb_to_gray(img):\n",
        "    return img.mean(dim=1, keepdim=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"colorization_model_mse.pth\"))\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gyVeA5ZeSFu",
        "outputId": "884c9489-350d-4054-b1bf-ffc2e3a430e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ColorizationNet(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2))\n",
              "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2))\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2))\n",
              "  (conv4): Conv2d(128, 3, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGCn66P_KvaT"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "EPOCHS = 100\n",
        "for epoch in range(EPOCHS):\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        grayscale_images = rgb_to_gray(images).to(device)\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "\n",
        "        outputs = model(grayscale_images)\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Finished Training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'historical_colorization_model.pth')"
      ],
      "metadata": {
        "id": "o3T_VhWRleDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Klmj5iKK3ZU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "    # Convert from Tensor image and display\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if len(img.shape) == 2:  # grayscale image\n",
        "        plt.imshow(npimg, cmap='gray')\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualize_all_three(original_images, grayscale_images, colorized_images, n=5):\n",
        "    \"\"\"\n",
        "    Display grayscale, colorized, and original images side by side.\n",
        "    n: number of images to display from the batch\n",
        "    \"\"\"\n",
        "    num_images = min(len(original_images), len(grayscale_images), len(colorized_images), n)\n",
        "    n=num_images\n",
        "    fig = plt.figure(figsize=(3*n, 4))\n",
        "    for i in range(n):\n",
        "        # Display original image\n",
        "        ax = plt.subplot(1, 3*n, 3*i + 1)\n",
        "        imshow(original_images[i])\n",
        "        ax.set_title(\"Original\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Display original grayscale image\n",
        "        ax = plt.subplot(1, 3*n, 3*i + 2)\n",
        "        imshow(grayscale_images[i])\n",
        "        ax.set_title(\"Grayscale\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Display colorized image\n",
        "        ax = plt.subplot(1, 3*n, 3*i + 3)\n",
        "        imshow(colorized_images[i])\n",
        "        ax.set_title(\"Colorized\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def torch_rgb_to_hsv(rgb):\n",
        "    \"\"\"\n",
        "    Convert an RGB image tensor to HSV.\n",
        "\n",
        "    Parameters:\n",
        "    - rgb: tensor of shape (batch_size, 3, height, width) in RGB format in the range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "    - hsv: tensor of same shape in HSV format in the range [0, 1].\n",
        "    \"\"\"\n",
        "    r, g, b = rgb[:, 0, :, :], rgb[:, 1, :, :], rgb[:, 2, :, :]\n",
        "    max_val, _ = torch.max(rgb, dim=1)\n",
        "    min_val, _ = torch.min(rgb, dim=1)\n",
        "    diff = max_val - min_val\n",
        "\n",
        "    # Compute H\n",
        "    h = torch.zeros_like(r)\n",
        "    mask = (max_val == r) & (g >= b)\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask]\n",
        "    mask = (max_val == r) & (g < b)\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask] + 6.0\n",
        "    mask = max_val == g\n",
        "    h[mask] = (b[mask] - r[mask]) / diff[mask] + 2.0\n",
        "    mask = max_val == b\n",
        "    h[mask] = (r[mask] - g[mask]) / diff[mask] + 4.0\n",
        "    h = h / 6.0\n",
        "    h[diff == 0.0] = 0.0\n",
        "\n",
        "    # Compute S\n",
        "    s = torch.zeros_like(r)\n",
        "    s[diff != 0.0] = diff[diff != 0.0] / max_val[diff != 0.0]\n",
        "\n",
        "    # V is just max_val\n",
        "    v = max_val\n",
        "\n",
        "    return torch.stack([h, s, v], dim=1)\n",
        "\n",
        "\n",
        "def torch_hsv_to_rgb(hsv):\n",
        "    \"\"\"\n",
        "    Convert an HSV image tensor to RGB.\n",
        "\n",
        "    Parameters:\n",
        "    - hsv: tensor of shape (batch_size, 3, height, width) in HSV format in the range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "    - rgb: tensor of same shape in RGB format in the range [0, 1].\n",
        "    \"\"\"\n",
        "    h, s, v = hsv[:, 0, :, :], hsv[:, 1, :, :], hsv[:, 2, :, :]\n",
        "    i = (h * 6.0).floor()\n",
        "    f = h * 6.0 - i\n",
        "    p = v * (1.0 - s)\n",
        "    q = v * (1.0 - s * f)\n",
        "    t = v * (1.0 - s * (1.0 - f))\n",
        "\n",
        "    i_mod = i % 6\n",
        "    r = torch.zeros_like(h)\n",
        "    g = torch.zeros_like(h)\n",
        "    b = torch.zeros_like(h)\n",
        "\n",
        "    r[i_mod == 0.0] = v[i_mod == 0.0]\n",
        "    g[i_mod == 0.0] = t[i_mod == 0.0]\n",
        "    b[i_mod == 0.0] = p[i_mod == 0.0]\n",
        "\n",
        "    r[i_mod == 1.0] = q[i_mod == 1.0]\n",
        "    g[i_mod == 1.0] = v[i_mod == 1.0]\n",
        "    b[i_mod == 1.0] = p[i_mod == 1.0]\n",
        "\n",
        "    r[i_mod == 2.0] = p[i_mod == 2.0]\n",
        "    g[i_mod == 2.0] = v[i_mod == 2.0]\n",
        "    b[i_mod == 2.0] = t[i_mod == 2.0]\n",
        "\n",
        "    r[i_mod == 3.0] = p[i_mod == 3.0]\n",
        "    g[i_mod == 3.0] = q[i_mod == 3.0]\n",
        "    b[i_mod == 3.0] = v[i_mod == 3.0]\n",
        "\n",
        "    r[i_mod == 4.0] = t[i_mod == 4.0]\n",
        "    g[i_mod == 4.0] = p[i_mod == 4.0]\n",
        "    b[i_mod == 4.0] = v[i_mod == 4.0]\n",
        "\n",
        "    r[i_mod == 5.0] = v[i_mod == 5.0]\n",
        "    g[i_mod == 5.0] = p[i_mod == 5.0]\n",
        "    b[i_mod == 5.0] = q[i_mod == 5.0]\n",
        "\n",
        "    return torch.stack([r, g, b], dim=1)\n",
        "\n",
        "def exaggerate_colors(images, saturation_factor=1.5, value_factor=1.2):\n",
        "    \"\"\"\n",
        "    Exaggerate the colors of RGB images.\n",
        "\n",
        "    Parameters:\n",
        "    - images: tensor of shape (batch_size, 3, height, width) in RGB format.\n",
        "    - saturation_factor: factor by which to increase the saturation. Default is 1.5.\n",
        "    - value_factor: factor by which to increase the value/brightness. Default is 1.2.\n",
        "\n",
        "    Returns:\n",
        "    - color_exaggerated_images: tensor of same shape as input, with exaggerated colors.\n",
        "    \"\"\"\n",
        "    # Convert images to the range [0, 1]\n",
        "    images = (images + 1) / 2.0\n",
        "\n",
        "    # Convert RGB images to HSV\n",
        "    images_hsv = torch_rgb_to_hsv(images)\n",
        "\n",
        "    # Increase the saturation and value components\n",
        "    images_hsv[:, 1, :, :] = torch.clamp(images_hsv[:, 1, :, :] * saturation_factor, 0, 1)\n",
        "    images_hsv[:, 2, :, :] = torch.clamp(images_hsv[:, 2, :, :] * value_factor, 0, 1)\n",
        "\n",
        "    # Convert the modified HSV images back to RGB\n",
        "    color_exaggerated_images = torch_hsv_to_rgb(images_hsv)\n",
        "\n",
        "    # Convert images back to the range [-1, 1]\n",
        "    color_exaggerated_images = color_exaggerated_images * 2.0 - 1.0\n",
        "\n",
        "    return color_exaggerated_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx3P-w_UN6KR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    for i, (images, _) in enumerate(test_loader):\n",
        "        grayscale_images = rgb_to_gray(images).to(device)\n",
        "        colorized_images = model(grayscale_images)\n",
        "\n",
        "\n",
        "        grayscale_images_cpu = grayscale_images.cpu().squeeze(1)\n",
        "        colorized_images_cpu = colorized_images.cpu()\n",
        "        original_images_cpu = images.cpu()\n",
        "\n",
        "        #colorized_images_cpu=scale_predicted_colors(colorized_images_cpu)\n",
        "        colorized_images_cpu=exaggerate_colors(colorized_images_cpu)\n",
        "\n",
        "        # Visualize the grayscale, colorized, and original images\n",
        "        visualize_all_three(original_images_cpu, grayscale_images_cpu, colorized_images_cpu)\n",
        "\n",
        "        if i == 10:  # only do this for up to certain batch for demonstration purposes\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWGK8QhXGj9F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory:\", current_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln2mIcWjGzdw"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# List the uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    print(\"Uploaded file:\", filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1P9mdQFG9Yh",
        "outputId": "18adc4a1-1ed2-4905-c006-06aa35f799ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the current directory: ['.config', 'colorization_model_mse.pth', 'The_Royal_Air_Force_in_West_Africa,_March_1943_TR751.jpg', 'images', '.ipynb_checkpoints', 'historical_colorization_model.pth', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List all files in the current directory\n",
        "files = os.listdir(\"/content\")\n",
        "print(\"Files in the current directory:\", files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VcIY96J4btnp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Open the image. (Keep your image in the current directory. In my case, the image was horse.jpg)\n",
        "img = Image.open(filename)\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray_img = img.convert(\"L\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mwG9M2w_b4UK"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # If you need to normalize, uncomment the following line\n",
        "    # transforms.Normalize(mean=[0.5], std=[0.5])  # Assuming you want to normalize to [-1, 1] range\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dw32IQdrN99Z"
      },
      "outputs": [],
      "source": [
        "# Apply the transformations\n",
        "img_tensor = transform(gray_img).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move the image tensor to the device where your model is (likely 'cuda' if using GPU)\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "# Get the model's output\n",
        "with torch.no_grad():\n",
        "    colorized_tensor = model(img_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "m-TZHaRmbzcn"
      },
      "outputs": [],
      "source": [
        "# Convert the tensor back to an image\n",
        "colorized_img = transforms.ToPILImage()(colorized_tensor.squeeze(0).cpu())\n",
        "\n",
        "# Optionally, save the image\n",
        "colorized_img.save(\"_colorized2.jpg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBgXxEUw_spt"
      },
      "outputs": [],
      "source": [
        "# Plotting the original, grayscale, and colorized images side-by-side\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 6))  # Create a figure with 1 row and 3 columns\n",
        "\n",
        "# Display original color image\n",
        "ax[0].imshow(img)\n",
        "ax[0].set_title(\"Original Color Image\")\n",
        "ax[0].axis('off')  # Hide axes\n",
        "\n",
        "# Display grayscale image\n",
        "ax[1].imshow(gray_img, cmap='gray')  # Since it's grayscale, use cmap='gray'\n",
        "ax[1].set_title(\"Grayscale Image\")\n",
        "ax[1].axis('off')  # Hide axes\n",
        "\n",
        "# Display colorized image\n",
        "ax[2].imshow(colorized_img)\n",
        "ax[2].set_title(\"Colorized Image\")\n",
        "ax[2].axis('off')  # Hide axes\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9ttCuiL-r9Yb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "total_mse = 0\n",
        "num_images = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        grayscale_images = rgb_to_gray(images).to(device)\n",
        "        colorized_images = model(grayscale_images)\n",
        "\n",
        "        # Convert tensors to NumPy arrays and flatten\n",
        "        original_images_np = images.cpu().numpy().flatten()\n",
        "        colorized_images_np = colorized_images.cpu().numpy().flatten()\n",
        "\n",
        "        # Calculate MSE for current batch\n",
        "        batch_mse = mean_squared_error(original_images_np, colorized_images_np)\n",
        "\n",
        "        # Accumulate MSE and image count\n",
        "        total_mse += batch_mse * images.size(0)\n",
        "        num_images += images.size(0)\n",
        "\n",
        "# Calculate average MSE\n",
        "average_mse = total_mse / num_images\n",
        "\n",
        "# Print the average MSE\n",
        "print(f\"Average MSE: {average_mse:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkflQ0IAr93p",
        "outputId": "623e1261-286c-4665-812a-a32d4611d4aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE: 0.0013\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}